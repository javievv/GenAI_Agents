{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LangGraph 的科学论文代理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "\n",
    "\n",
    "本项目实现了一个智能研究助手，使用 LangGraph 和先进的语言模型帮助用户导航、理解和分析科学文献。通过将各种学术 API 与复杂的论文处理技术相结合，它为研究人员、学生和处理学术论文的专业人士创造了无缝体验。\n",
    "\n",
    "> 注意：所展示的工作流程并非特定于某一领域：图表中的每一步都可以通过简单地更改提示来适应不同的领域。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动机\n",
    "\n",
    "研究文献综述在研发中占据了大量时间，研究表明研究人员花费 30-50% 的时间阅读、分析和综合学术论文。这一挑战在整个研究界普遍存在。虽然彻底的文献综述对于推动科学技术进步至关重要，但当前的过程仍然效率低下且耗时。\n",
    "\n",
    "主要挑战包括：\n",
    "- 大量时间投入（30-50% 的研发时间）用于阅读和处理论文\n",
    "- 跨越分散的数据库生态系统的低效搜索过程\n",
    "- 综合和连接多篇论文发现的复杂任务\n",
    "- 维护全面文献综述的资源密集型工作\n",
    "- 持续努力以跟上新发表的文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键组件\n",
    "\n",
    " 1. 状态驱动的工作流引擎 \n",
    "    - 状态图架构：用于协调研究的五节点系统 \n",
    "    - 决策节点：查询意图分析和路由 \n",
    "    - 规划节点：研究策略制定\n",
    "    - 工具执行节点：论文检索和处理 \n",
    "    - 评判节点：质量验证和改进周期 \n",
    "\n",
    "2. 论文处理集成 \n",
    "    - 源集成，用于全面论文访问的 CORE API \n",
    "    - 文档处理，PDF 内容提取，文本结构保存 \n",
    "\n",
    "3. 分析工作流 \n",
    "    - 状态感知处理管道 \n",
    "    - 多步验证门控 \n",
    "    - 以质量为中心的改进周期 \n",
    "    - 人机回环验证选项\n",
    "\n",
    "工作流程概览如下所示：\n",
    "\n",
    "![图片](https://i.ibb.co/0BBzkcb/mermaid-diagram-2024-11-17-195744.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法详情\n",
    "\n",
    "1. 系统需要 \n",
    "    - OpenAI API 密钥以访问 GPT 4o。该模型是在与其他开源替代方案（特别是 Llama 3）进行比较后选定的。但是，可以使用任何其他具有工具调用能力的 LLM。\n",
    "    - 用于论文检索的 CORE API 密钥。CORE 是最大的在线科学论文库之一，拥有超过 1.36 亿篇论文，并为个人使用提供免费 API。可以在[这里](https://core.ac.uk/services/api#form)申请密钥。\n",
    "\n",
    "2. 技术架构：\n",
    "    - 用于状态编排的 LangGraph。\n",
    "    - 用于文档处理的 PDFplumber。\n",
    "    - 用于结构化数据处理的 Pydantic。\n",
    "\n",
    "> 致谢：特别感谢 CORE API 实现了学术论文访问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置\n",
    "\n",
    "此单元格安装所需的依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet langchain==0.2.16 langchain-community==0.2.16 langchain-openai==0.1.23 langgraph==0.2.18 langsmith==0.1.114 pdfplumber python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此单元格导入所需的库并设置环境变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, ClassVar, Sequence, TypedDict, Optional\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "load_dotenv()\n",
    "\n",
    "# You can set your own keys here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"CORE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提示\n",
    "\n",
    "此单元格包含工作流中使用的提示。\n",
    "\n",
    "`agent_prompt` 包含解释如何使用 CORE API 进行复杂查询的部分，使代理能够解决更复杂的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the initial decision making on how to reply to the user\n",
    "decision_making_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the user with their scientific research.\n",
    "\n",
    "Based on the user query, decide if you need to perform a research or if you can answer the question directly.\n",
    "- You should perform a research if the user query requires any supporting evidence or information.\n",
    "- You should answer the question directly only for simple conversational questions, like \"how are you?\".\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to create a step by step plan to answer the user query\n",
    "planning_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to make a new step by step plan to help the user with their scientific research .\n",
    "\n",
    "Subtasks should not rely on any assumptions or guesses, but only rely on the information provided in the context or look up for any additional information.\n",
    "\n",
    "If any feedback is provided about a previous answer, incorportate it in your new planning.\n",
    "\n",
    "\n",
    "# TOOLS\n",
    "\n",
    "For each subtask, indicate the external tool required to complete the subtask. \n",
    "Tools can be one of the following:\n",
    "{tools}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the agent to answer the user query\n",
    "agent_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher. \n",
    "Your goal is to help the user with their scientific research. You have access to a set of external tools to complete your tasks.\n",
    "Follow the plan you wrote to successfully complete the task.\n",
    "\n",
    "Add extensive inline citations to support any claim made in the answer.\n",
    "\n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## CORE API\n",
    "\n",
    "The CORE API has a specific query language that allows you to explore a vast papers collection and perform complex queries. See the following table for a list of available operators:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "Use this table to formulate more complex queries filtering for specific papers, for example publication date/year.\n",
    "Here are the relevant fields of a paper object you can use to filter the results:\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "Example queries:\n",
    "- \"machine learning AND yearPublished:2023\"\n",
    "- \"maritime biology AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "- \"cancer research AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "- \"title:Attention is all you need\"\n",
    "- \"mathematics AND _exists_:abstract\"\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the judging step to evaluate the quality of the final answer\n",
    "judge_prompt = \"\"\"\n",
    "You are an expert scientific researcher.\n",
    "Your goal is to review the final answer you provided for a specific user query.\n",
    "\n",
    "Look at the conversation history between you and the user. Based on it, you need to decide if the final answer is satisfactory or not.\n",
    "\n",
    "A good final answer should:\n",
    "- Directly answer the user query. For example, it does not answer a question about a different paper or area of research.\n",
    "- Answer extensively the request from the user.\n",
    "- Take into account any feedback given through the conversation.\n",
    "- Provide inline sources to support any claim made in the answer.\n",
    "\n",
    "In case the answer is not good enough, provide clear and concise feedback on what needs to be improved to pass the evaluation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实用类和函数\n",
    "\n",
    "此单元格包含工作流中使用的实用类和函数。它包括 CORE API 的包装器、节点输入和输出的 Pydantic 模型以及一些通用函数。\n",
    "\n",
    "`CoreAPIWrapper` 类包含重试机制，以处理瞬态错误并使工作流更加健壮。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAPIWrapper(BaseModel):\n",
    "    \"\"\"Simple wrapper around the CORE API.\"\"\"\n",
    "    base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"\n",
    "    api_key: ClassVar[str] = os.environ[\"CORE_API_KEY\"]\n",
    "\n",
    "    top_k_results: int = Field(description = \"Top k results obtained by running a query on Core\", default = 1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> dict:\n",
    "        http = urllib3.PoolManager()\n",
    "\n",
    "        # Retry mechanism to handle transient errors\n",
    "        max_retries = 5    \n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request(\n",
    "                'GET',\n",
    "                f\"{self.base_url}/search/outputs\", \n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"}, \n",
    "                fields={\"q\": query, \"limit\": self.top_k_results}\n",
    "            )\n",
    "            if 200 <= response.status < 300:\n",
    "                return response.json()\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx response from CORE API: {response.status} {response.data}\")\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        response = self._get_search_response(query)\n",
    "        results = response.get(\"results\", [])\n",
    "        if not results:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        # Format the results in a string\n",
    "        docs = []\n",
    "        for result in results:\n",
    "            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            docs.append((\n",
    "                f\"* ID: {result.get('id', '')},\\n\"\n",
    "                f\"* Title: {result.get('title', '')},\\n\"\n",
    "                f\"* Published Date: {published_date_str},\\n\"\n",
    "                f\"* Authors: {authors_str},\\n\"\n",
    "                f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "                f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "            ))\n",
    "        return \"\\n-----\\n\".join(docs)\n",
    "\n",
    "class SearchPapersInput(BaseModel):\n",
    "    \"\"\"Input object to search papers with the CORE API.\"\"\"\n",
    "    query: str = Field(description=\"The query to search for on the selected archive.\")\n",
    "    max_papers: int = Field(description=\"The maximum number of papers to return. It's default to 1, but you can increase it up to 10 in case you need to perform a more comprehensive search.\", default=1, ge=1, le=10)\n",
    "\n",
    "class DecisionMakingOutput(BaseModel):\n",
    "    \"\"\"Output object of the decision making node.\"\"\"\n",
    "    requires_research: bool = Field(description=\"Whether the user query requires research or not.\")\n",
    "    answer: Optional[str] = Field(default=None, description=\"The answer to the user query. It should be None if the user query requires research, otherwise it should be a direct answer to the user query.\")\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    \"\"\"Output object of the judge node.\"\"\"\n",
    "    is_good_answer: bool = Field(description=\"Whether the answer is good or not.\")\n",
    "    feedback: Optional[str] = Field(default=None, description=\"Detailed feedback about why the answer is not good. It should be None if the answer is good.\")\n",
    "\n",
    "def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "async def print_stream(app: CompiledStateGraph, input: str) -> Optional[BaseMessage]:\n",
    "    display(Markdown(\"## New research running\"))\n",
    "    display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "    # Stream the results \n",
    "    all_messages = []\n",
    "    async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "        for updates in chunk.values():\n",
    "            if messages := updates.get(\"messages\"):\n",
    "                all_messages.extend(messages)\n",
    "                for message in messages:\n",
    "                    message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    " \n",
    "    # Return the last message if any\n",
    "    if not all_messages:\n",
    "        return None\n",
    "    return all_messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代理状态\n",
    "\n",
    "此单元格定义了代理状态，其中包含以下信息：\n",
    "- `requires_research`：用户查询是否需要研究。\n",
    "- `num_feedback_requests`：LLM 询问反馈的次数。\n",
    "- `is_good_answer`：LLM 的最终答案是否良好。\n",
    "- `messages`：用户与 LLM 之间的对话历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent during the paper research process.\"\"\"\n",
    "    requires_research: bool = False\n",
    "    num_feedback_requests: int = 0\n",
    "    is_good_answer: bool = False\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代理工具\n",
    "\n",
    "此单元格定义了代理可用的工具。工具包包含使用 CORE API 搜索科学论文的工具、从给定 URL 下载科学论文的工具以及请求人工反馈的工具。\n",
    "\n",
    "为了使论文下载更加稳健，该工具包含重试机制（类似于 CORE API 使用的机制）以及模拟浏览器标头以避免 403 错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"Search for scientific papers using the CORE API.\n",
    "\n",
    "    Example:\n",
    "    {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "\n",
    "    Returns:\n",
    "        A list of the relevant papers found with the corresponding relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return CoreAPIWrapper(top_k_results=max_papers).search(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error performing paper search: {e}\"\n",
    "\n",
    "@tool(\"download-paper\")\n",
    "def download_paper(url: str) -> str:\n",
    "    \"\"\"Download a specific scientific paper from a given URL.\n",
    "\n",
    "    Example:\n",
    "    {\"url\": \"https://sample.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        The paper content.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        http = urllib3.PoolManager(\n",
    "            cert_reqs='CERT_NONE',\n",
    "        )\n",
    "        \n",
    "        # Mock browser headers to avoid 403 error\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request('GET', url, headers=headers)\n",
    "            if 200 <= response.status < 300:\n",
    "                pdf_file = io.BytesIO(response.data)\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "@tool(\"ask-human-feedback\")\n",
    "def ask_human_feedback(question: str) -> str:\n",
    "    \"\"\"Ask for human feedback. You should call this tool when encountering unexpected errors.\"\"\"\n",
    "    return input(question)\n",
    "\n",
    "tools = [search_papers, download_paper, ask_human_feedback]\n",
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作流节点\n",
    "\n",
    "此单元格定义了工作流的节点。请注意，`judge_node` 被配置为如果 LLM 两次未能提供良好的答案，则结束执行，以保持延迟在可接受范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "base_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "decision_making_llm = base_llm.with_structured_output(DecisionMakingOutput)\n",
    "agent_llm = base_llm.bind_tools(tools)\n",
    "judge_llm = base_llm.with_structured_output(JudgeOutput)\n",
    "\n",
    "# Decision making node\n",
    "def decision_making_node(state: AgentState):\n",
    "    \"\"\"Entry point of the workflow. Based on the user query, the model can either respond directly or perform a full research, routing the workflow to the planning node\"\"\"\n",
    "    system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\"requires_research\": response.requires_research}\n",
    "    if response.answer:\n",
    "        output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "    return output\n",
    "\n",
    "# Task router function\n",
    "def router(state: AgentState):\n",
    "    \"\"\"Router directing the user query to the appropriate branch of the workflow.\"\"\"\n",
    "    if state[\"requires_research\"]:\n",
    "        return \"planning\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Planning node\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Planning node that creates a step by step plan to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "    response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Tool call node\n",
    "def tools_node(state: AgentState):\n",
    "    \"\"\"Tool call node that executes the tools based on the plan.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# Agent call node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=agent_prompt)\n",
    "    response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Should continue function\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the agent should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # End execution if there are no tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Judge node\n",
    "def judge_node(state: AgentState):\n",
    "    \"\"\"Node to let the LLM judge the quality of its own final answer.\"\"\"\n",
    "    # End execution if the LLM failed to provide a good answer twice.\n",
    "    num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "    if num_feedback_requests >= 2:\n",
    "        return {\"is_good_answer\": True}\n",
    "\n",
    "    system_prompt = SystemMessage(content=judge_prompt)\n",
    "    response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\n",
    "        \"is_good_answer\": response.is_good_answer,\n",
    "        \"num_feedback_requests\": num_feedback_requests + 1\n",
    "    }\n",
    "    if response.feedback:\n",
    "        output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "    return output\n",
    "\n",
    "# Final answer router function\n",
    "def final_answer_router(state: AgentState):\n",
    "    \"\"\"Router to end the workflow or improve the answer.\"\"\"\n",
    "    if state[\"is_good_answer\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"planning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作流定义\n",
    "\n",
    "此单元格使用 LangGraph 定义工作流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"decision_making\", decision_making_node)\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"tools\", tools_node)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"judge\", judge_node)\n",
    "\n",
    "# Set the entry point of the graph\n",
    "workflow.set_entry_point(\"decision_making\")\n",
    "\n",
    "# Add edges between nodes\n",
    "workflow.add_conditional_edges(\n",
    "    \"decision_making\",\n",
    "    router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"planning\", \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"judge\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    final_answer_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 博士学术研究用例示例\n",
    "\n",
    "此单元格使用几个示例查询测试工作流。这些查询旨在从以下方面评估代理：\n",
    "- 完成代表博士研究人员可能需要执行的工作的任务。\n",
    "- 解决需要在规定时间内研究论文的更具体任务。\n",
    "- 处理跨越多个研究领域的任务。\n",
    "- 通过从论文中获取特定信息来批判性地评估其自身的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    \"Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\",\n",
    "\n",
    "    \"Can you find 8 papers on quantum machine learning?\",\n",
    "\n",
    "    \"\"\"Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
    "    focusing on clinical trials and safety protocols\"\"\",\n",
    "\n",
    "    \"\"\"Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
    "    specifically looking for novel architectural modifications with experimental validation.\"\"\"\n",
    "]\n",
    "\n",
    "# Run tests and store the results for later visualisation\n",
    "outputs = []\n",
    "for test_input in test_inputs:\n",
    "    final_answer = await print_stream(app, test_input)\n",
    "    outputs.append(final_answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 显示结果\n",
    "\n",
    "此单元格显示测试查询的结果，以便更紧凑地可视化结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Download and summarize the findings of this paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11379842/pdf/11671_2024_Article_4070.pdf\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "The paper titled \"Advances, limitations and perspectives in the use of celecoxib-loaded nanocarriers in therapeutics of cancer\" reviews the development and potential of celecoxib (CXB)-loaded nanocarriers in cancer treatment. Celecoxib is a selective COX-2 inhibitor used in cancer therapy, but its use is limited by the need for high doses, which can cause severe side effects. Nanocarriers offer a promising solution by improving the drug's biopharmaceutical properties, allowing for controlled release and targeted delivery.\n",
       "\n",
       "### Key Findings:\n",
       "\n",
       "1. **Nanocarrier Types and Materials**: \n",
       "   - CXB-loaded nanocarriers are primarily based on polymers and lipids, using materials like poly(lactic-co-glycolic acid) (PLGA), cholesterol, phospholipids, and poly(ethylene glycol) (PEG).\n",
       "   - These carriers enhance drug solubility, stability, and bioavailability, and can be engineered for targeted delivery to tumor sites.\n",
       "\n",
       "2. **Advancements in Formulations**:\n",
       "   - Recent developments include the use of cell surface ligands, co-delivery of synergistic agents, and materials that provide imaging capabilities.\n",
       "   - The combination of CXB with other anti-inflammatory drugs or apoptosis inducers shows promise in enhancing therapeutic effects.\n",
       "\n",
       "3. **Clinical and Preclinical Studies**:\n",
       "   - The research is mostly in preclinical stages, with no current clinical trials using CXB-loaded nanocarriers for cancer treatment.\n",
       "   - In vivo studies have increased since 2017, indicating progress towards potential clinical applications.\n",
       "\n",
       "4. **Challenges and Future Directions**:\n",
       "   - The main challenges include CXB's low water solubility and the complexity of scaling up nanocarrier production for clinical use.\n",
       "   - Future research should focus on optimizing nanocarrier design for stability, targeting, and controlled release, as well as exploring synergistic drug combinations.\n",
       "\n",
       "5. **Potential Impact**:\n",
       "   - CXB-loaded nanocarriers could significantly enhance the efficacy of cancer treatments by improving drug delivery and reducing side effects.\n",
       "   - The ability of CXB to potentiate the effects of established chemotherapeutic agents is a major clinical advancement.\n",
       "\n",
       "The paper highlights the potential of nanotechnology to revolutionize cancer therapy by enabling more effective and less harmful treatment options through the use of CXB-loaded nanocarriers.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Can you find 8 papers on quantum machine learning?\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are 8 papers on quantum machine learning:\n",
       "\n",
       "1. **Quantum Circuit Learning**\n",
       "   - **Authors**: Mitarai, Kosuke; Negoro, Makoto; Kitagawa, Masahiro; Fujii, Keisuke\n",
       "   - **Published Date**: April 23, 2019\n",
       "   - **Abstract**: This paper proposes a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, called quantum circuit learning. The framework allows a quantum circuit to learn tasks by tuning parameters, circumventing high-depth circuits. Theoretical and numerical simulations show that a quantum circuit can approximate nonlinear functions.\n",
       "   - **URL**: [Quantum Circuit Learning](http://arxiv.org/abs/1803.00745)\n",
       "\n",
       "2. **Quantum Machine Learning**\n",
       "   - **Authors**: Biamonte, Jacob; Wittek, Peter; Pancotti, Nicola; Rebentrost, Patrick; Wiebe, Nathan; Lloyd, Seth\n",
       "   - **Published Date**: May 10, 2018\n",
       "   - **Abstract**: This paper explores the potential of quantum computers to outperform classical computers on machine learning tasks. It discusses the challenges and paths towards solutions in quantum machine learning.\n",
       "   - **URL**: [Quantum Machine Learning](http://arxiv.org/abs/1611.09347)\n",
       "\n",
       "3. **The Power of One Qubit in Machine Learning**\n",
       "   - **Authors**: Ghobadi, Roohollah; Oberoi, Jaspreet S.; Zahedinejhad, Ehsan\n",
       "   - **Published Date**: June 8, 2019\n",
       "   - **Abstract**: This paper proposes a kernel-based quantum machine learning algorithm that can be implemented on near-term quantum devices, using deterministic quantum computing with one qubit.\n",
       "   - **URL**: [The Power of One Qubit in Machine Learning](http://arxiv.org/abs/1905.01390)\n",
       "\n",
       "4. **Quantum Machine Learning: A Classical Perspective**\n",
       "   - **Authors**: Ciliberto, Carlo; Herbster, Mark; Ialongo, Alessandro Davide; Pontil, Massimiliano; Rocchetto, Andrea; Severini, Simone; Wossnig, Leonard\n",
       "   - **Published Date**: February 13, 2018\n",
       "   - **Abstract**: This review discusses the potential of quantum computation to speed up classical machine learning algorithms, highlighting the limitations and advantages of quantum resources for learning problems.\n",
       "   - **URL**: [Quantum Machine Learning: A Classical Perspective](http://arxiv.org/abs/1707.08561)\n",
       "\n",
       "5. **Quantum Machine Learning Over Infinite Dimensions**\n",
       "   - **Authors**: Lau, Hoi-Kwan; Pooser, Raphael; Siopsis, George; Weedbrook, Christian\n",
       "   - **Published Date**: November 14, 2016\n",
       "   - **Abstract**: This paper generalizes quantum machine learning to infinite-dimensional systems, presenting subroutines for quantum machine learning algorithms on continuous-variable quantum computers.\n",
       "   - **URL**: [Quantum Machine Learning Over Infinite Dimensions](http://arxiv.org/abs/1603.06222)\n",
       "\n",
       "6. **Experimental Demonstration of Quantum Learning Speed-up with Classical Input Data**\n",
       "   - **Authors**: Lee, Joong-Sung; Bang, Jeongho; Hong, Sunghyuk; Lee, Changhyoup; Seol, Kang Hee; Lee, Jinhyoung; Lee, Kwang-Geol\n",
       "   - **Published Date**: November 22, 2018\n",
       "   - **Abstract**: This paper demonstrates a quantum-classical hybrid machine learning approach, showing a quantum learning speed-up of approximately 36% compared to classical machines.\n",
       "   - **URL**: [Experimental Demonstration of Quantum Learning Speed-up](http://arxiv.org/abs/1706.01561)\n",
       "\n",
       "7. **Quantum-Enhanced Machine Learning**\n",
       "   - **Authors**: Dunjko, Vedran; Taylor, Jacob M.; Briegel, Hans J.\n",
       "   - **Published Date**: October 26, 2016\n",
       "   - **Abstract**: This work proposes a systematic approach to machine learning from the perspective of quantum information, covering supervised, unsupervised, and reinforcement learning.\n",
       "   - **URL**: [Quantum-Enhanced Machine Learning](http://arxiv.org/abs/1610.08251)\n",
       "\n",
       "8. **An Efficient Quantum Algorithm for Generative Machine Learning**\n",
       "   - **Authors**: Gao, Xun; Zhang, Zhengyu; Duan, Luming\n",
       "   - **Published Date**: November 6, 2017\n",
       "   - **Abstract**: This paper proposes a quantum algorithm for generative machine learning, showing exponential improvements in training and inference over classical algorithms.\n",
       "   - **URL**: [An Efficient Quantum Algorithm for Generative Machine Learning](http://arxiv.org/abs/1711.02038)\n",
       "\n",
       "These papers cover a range of topics within quantum machine learning, from theoretical frameworks to experimental demonstrations.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find recent papers (2023-2024) about CRISPR applications in treating genetic disorders, \n",
       "    focusing on clinical trials and safety protocols\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are some recent papers (2023-2024) on CRISPR applications in treating genetic disorders, focusing on clinical trials and safety protocols:\n",
       "\n",
       "1. **CRISPR-Cas9 Gene Editing Tool: Potential Treatment for Sickle Cell Disease**\n",
       "   - **Authors**: Young, Brittany\n",
       "   - **Published Date**: April 25, 2024\n",
       "   - **Abstract**: Not provided.\n",
       "   - **URL**: [Read the paper](https://digitalcommons.sacredheart.edu/cgi/viewcontent.cgi?article=2403&context=acadfest)\n",
       "\n",
       "2. **Progress and Harmonization of Gene Editing to Treat Human Diseases: Proceeding of COST Action CA21113 GenE-HumDi**\n",
       "   - **Authors**: Cavazza, Alessia; González Martínez, Coral; Sánchez Martín, Rosario María; Martín Molina, Francisco; Benabdellah, Karim; COST Action CA21113\n",
       "   - **Published Date**: December 12, 2023\n",
       "   - **Abstract**: This publication discusses the efforts of the GenE-HumDi network to expedite the application of genome editing for therapeutic purposes in treating human diseases. It covers aspects like enhancing genome editing technologies, assessing delivery systems, addressing safety concerns, promoting clinical translation, and developing regulatory guidelines.\n",
       "   - **URL**: [Read the paper](https://digibug.ugr.es/bitstream/10481/86007/1/1-s2.0-S2162253123002846-main.pdf)\n",
       "\n",
       "3. **Germline Genome Editing of Human IVF Embryos Should Not Be Subject to Overly Stringent Restrictions**\n",
       "   - **Authors**: Smith, Kevin\n",
       "   - **Published Date**: July 5, 2024\n",
       "   - **Abstract**: This paper critiques the restrictive criteria for germline genome editing, advocating for a balanced approach that weighs potential benefits against risks. It suggests that ethical oversight combined with genetic scrutiny can enable responsible use of the technology.\n",
       "   - **URL**: [Read the paper](https://rke.abertay.ac.uk/files/81349925/Smith_GermlineGenomeEditing_Publised_2024.pdf)\n",
       "\n",
       "4. **Balancing Progress and Ethics: Exploring the Science and Ethics of Gene Editing: Literature Review**\n",
       "   - **Authors**: Burgess, Jackson\n",
       "   - **Published Date**: January 1, 2024\n",
       "   - **Abstract**: Not provided.\n",
       "   - **URL**: [Read the paper](https://scholarworks.uni.edu/cgi/viewcontent.cgi?article=1929&context=hpt)\n",
       "\n",
       "These papers provide insights into the current state of CRISPR technology in clinical settings, focusing on safety protocols and ethical considerations.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find and analyze papers from 2023-2024 about the application of transformer architectures in protein folding prediction, \n",
       "    specifically looking for novel architectural modifications with experimental validation.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "I found several papers from 2023 that discuss the application of transformer architectures in protein folding prediction, with a focus on novel architectural modifications and experimental validation. Here are some of the most relevant papers:\n",
       "\n",
       "1. **Protein tertiary structure prediction and refinement using deep learning**\n",
       "   - **Authors**: Wu, Tianqi\n",
       "   - **Published Date**: 2023-01-08\n",
       "   - **Abstract**: This paper discusses the development of a method called TransPross, which applies a 1D transformer network and attention mechanism for protein secondary structure prediction. It also introduces ATOMRefine, a novel end-to-end protein structure refinement tool. The paper emphasizes the use of deep learning techniques in improving protein structure prediction.\n",
       "   - **URL**: [Read the paper](https://mospace.umsystem.edu/xmlui/bitstream/10355/94100/1/WuTianqiResearch.pdf)\n",
       "\n",
       "2. **Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation**\n",
       "   - **Authors**: Zhang, Le; Chen, Jiayang; Shen, Tao; Li, Yu; Sun, Siqi\n",
       "   - **Published Date**: 2023-06-02\n",
       "   - **Abstract**: This paper introduces MSA-Augmenter, a novel generative language model that uses protein-specific attention mechanisms to generate novel protein sequences. These sequences enhance the accuracy of structural property predictions, especially when existing sequences lack homologous families.\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2306.01824)\n",
       "\n",
       "3. **HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative**\n",
       "   - **Authors**: Fang, Xiaomin; Wang, Fan; Liu, Lihang; He, Jingzhou; Lin, Dayong; Xiang, Yingfei; Zhang, Xiaonan; Wu, Hua; Li, Hui; Song, Le\n",
       "   - **Published Date**: 2023-02-21\n",
       "   - **Abstract**: This paper presents HelixFold-Single, which combines a large-scale protein language model with AlphaFold2's geometric learning capabilities. It aims to predict protein structures using only primary sequences, bypassing the need for multiple sequence alignments (MSAs).\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2207.13921)\n",
       "\n",
       "4. **Integration of persistent Laplacian and pre-trained transformer for protein solubility changes upon mutation**\n",
       "   - **Authors**: Wee, JunJie; Chen, Jiahui; Xia, Kelin; Wei, Guo-Wei\n",
       "   - **Published Date**: 2023-11-02\n",
       "   - **Abstract**: This work integrates persistent Laplacians and pre-trained Transformers to predict protein solubility changes upon mutation. The model outperforms existing methods and improves the state-of-the-art by up to 15%.\n",
       "   - **URL**: [Read the paper](http://arxiv.org/abs/2310.18760)\n",
       "\n",
       "These papers provide insights into the latest advancements in using transformer architectures for protein folding prediction, with a focus on novel modifications and experimental validation. You can explore these papers further to gain a deeper understanding of the specific architectural innovations and their experimental outcomes.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input, output in zip(test_inputs, outputs):\n",
    "    display(Markdown(f\"## Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(f\"## Output:\\n\\n{output}\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较分析\n",
    "\n",
    "在这项综合分析中，我们将我们的科学论文代理与两个领先的 AI 知识副驾驶进行了评估：Microsoft Copilot 和 Perplexity AI。使用标准化查询 - “查找 8 篇关于量子机器学习的论文” - 我们进行了多维度的详细比较，以了解每个系统的优势、局限性和最佳用例。\n",
    "\n",
    "\n",
    "#### 测试用例实施\n",
    "\n",
    "我们在所有三个平台上使用相同的研究查询实施了受控测试：\n",
    "- 查询：“查找 8 篇关于量子机器学习的论文”\n",
    "- 样本量：多次测试运行以确保一致性\n",
    "- 评估时间：2024 年初\n",
    "- 跟踪指标：响应时间、元数据质量和结果结构\n",
    "\n",
    "#### 关键发现\n",
    "\n",
    "虽然我们的代理表现出卓越的学术严谨性和元数据完整性，每次查询大约需要 30 秒，但 Microsoft Copilot（2 秒）和 Perplexity AI（4-5 秒）等竞争对手在响应速度方面表现出优势。这种速度和深度之间的权衡反映了不同的设计理念和目标用例。\n",
    "\n",
    "比较分析揭示了方法的明显差异：\n",
    "- 我们的代理：针对具有全面验证的彻底学术研究进行了优化\n",
    "- Microsoft Copilot：专注于快速信息检索和一般概述\n",
    "- Perplexity AI：强调来源验证的平衡方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft copilot 结果 \n",
    "\n",
    "![image](https://i.ibb.co/y4Zf4Pc/Screenshot-2024-11-17-at-21-40-21.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity AI 结果\n",
    "![图片](https://i.ibb.co/n1rr7kW/Screenshot-2024-11-17-at-21-40-42.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指标比较\n",
    "![图片](https://i.ibb.co/5KbTmFq/Screenshot-2024-11-17-at-22-03-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们对我们的研究助理代理与领先平台（Microsoft Copilot 和 Perplexity AI）进行了全面比较。使用标准化查询 - “查找 8 篇关于量子机器学习的论文” - 我们评估了包括响应时间、元数据质量和学术价值在内的关键指标的性能。我们的分析揭示了明显的权衡：虽然我们的代理处理时间更长（30 秒 vs 2-5 秒），但它提供了明显更详细的元数据、经过验证的来源和结构化的学术输出。上面的比较表细分了多个维度的差异，帮助用户根据其特定的研究需求选择合适的工具——无论是快速探索（Copilot 擅长的地方）还是深入的学术研究（我们的代理展示其优势的地方）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 局限性\n",
    "\n",
    "1. 技术局限性\n",
    "    - 论文访问的 API 速率限制\n",
    "    - 大型 PDF 的处理时间\n",
    "    - 仅限于可公开访问的论文\n",
    "  \n",
    "2. 功能局限性\n",
    "    - 不支持论文中的图像分析\n",
    "    - 超长论文的上下文窗口有限\n",
    "    - 无法进行数学推导\n",
    "    - 非英语论文的语言限制\n",
    "\n",
    "\n",
    "## 潜在改进：\n",
    "\n",
    "1. 技术改进\n",
    "    - 实现多篇论文的并行处理\n",
    "    - 为经常访问的论文添加缓存系统\n",
    "    - 集成多个学术 API 以获得更广泛的覆盖范围\n",
    "    - 实现大型数据集的批处理\n",
    "\n",
    "2. 功能改进\n",
    "    - 添加对图形和表格提取的支持\n",
    "    - 实现论文之间的交叉引用\n",
    "    - 添加引文网络分析\n",
    "    - 包含特定领域的验证规则\n",
    "        \n",
    "3. 用户体验\n",
    "    - 添加交互式反馈机制\n",
    "    - 实现进度跟踪\n",
    "    - 添加可自定义的验证标准\n",
    "    - 包含研究摘要的导出选项\n",
    "        \n",
    "   \n",
    "## 具体用例：\n",
    "\n",
    "1. 学术研究、文献综述和论文分析。\n",
    "    - 综合搜索\n",
    "    - 引文跟踪\n",
    "    - 交叉引用验证\n",
    "\n",
    "2. 行业研究、技术文档和专利分析。\n",
    "    - 重点搜索\n",
    "    - 技术规范提取\n",
    "    - 竞争分析\n",
    "\n",
    "3. 教育、学生研究协助。\n",
    "    - 简化解释\n",
    "    - 学习资源识别\n",
    "    - 引导式研究过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "该实施演示了状态驱动架构如何改变学术论文分析。通过将 LangGraph 的编排能力与强大的 API 集成相结合，我们创建了一个系统，在自动化论文处理关键方面的同时保持研究的严谨性。工作流对验证和质量控制的重视确保了可靠的研究输出，同时显着简化了论文分析过程。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}